import { AIVoiceSettings } from "../types";

// Voice mapping from user voice names to ElevenLabs voice IDs
// ‚ö†Ô∏è CRITICAL: Keep this synchronized with lib/voiceProcessing.ts
export const VOICE_MAPPING: Record<string, string> = {
  // Core user voices (used in mockUsers.ts)
  "elevenlabs-aria": "9BWtsMINqrJLrRacOk9x",      // Aria - Female, warm
  "elevenlabs-domi": "TxGEqnHWrfWFTfGW9XjX",     // Domi - Male, professional (corrected)
  "elevenlabs-bella": "EXAVITQu4vr4xnSDxMaL",    // Bella - Female, soft
  "elevenlabs-echo": "pNInz6obpgDQGcFmaJgB",     // Echo - Male, deep (corrected)
  
  // Professional female voices (synchronized with lib/voiceProcessing.ts)
  "elevenlabs-rachel": "21m00Tcm4TlvDq8ikWAM",   // Rachel - Professional female
  "elevenlabs-sarah": "kdmDKE6EkgrWrrykO9Qt",    // Sarah - Professional female (corrected to Alexandra's realistic young female voice)
  "elevenlabs-emily": "AZnzlk1XvdvUeBnXmlld",    // Emily - Professional female
  
  // Professional male voices (synchronized with lib/voiceProcessing.ts)
  "elevenlabs-arnold": "pNInz6obpgDQGcFmaJgB",   // Arnold - Professional male (corrected)
  "elevenlabs-josh": "TxGEqnHWrfWFTfGW9XjX",     // Josh - Professional male
  "elevenlabs-sam": "yoZ06aMxZJJ28mfd3POQ",      // Sam - Professional male
  
  // Character voices (synchronized with lib/voiceProcessing.ts)
  "elevenlabs-dorothy": "ThT5KcBeYPX3keUQqHPh",  // Dorothy - Character voice
  "elevenlabs-charlie": "2EiwWnXFnvU5JabPnv8n",  // Charlie - Character voice (corrected)
  "elevenlabs-lily": "EXAVITQu4vr4xnSDxMaL",     // Lily - Character voice
  "elevenlabs-tommy": "yoZ06aMxZJJ28mfd3POQ",    // Tommy - Character voice
  
  // Extended voices (legacy mapping)
  "elevenlabs-onyx": "pNInz6obpgDQGcFmaJgB",     // Onyx - Male, serious
  "elevenlabs-nova": "piTKgcLEGmPE4e6mEKli",     // Nova - Female, clear
  "elevenlabs-shimmer": "VR6AewLTigWG4xSOukaG",  // Shimmer - Female, bright
  "elevenlabs-cyber": "2EiwWnXFnvU5JabPnv8n",    // Cyber - Male, robotic
  "elevenlabs-cosmo": "oWAxZDx7w5VEj9dCyTzz",    // Cosmo - Male, friendly
  "elevenlabs-scarlet": "wViIeQp4X7XZg8fAllTk",  // Scarlet - Female, passionate
};

// Get ElevenLabs voice ID from user voice name
export const getVoiceId = (userVoice: string): string => {
  const voiceId = VOICE_MAPPING[userVoice];
  if (!voiceId) {
    console.warn(`‚ö†Ô∏è Voice mapping not found for: ${userVoice}, using default Aria`);
    return "9BWtsMINqrJLrRacOk9x"; // Default to Aria
  }
  console.log(`üé§ Voice mapping: ${userVoice} ‚Üí ${voiceId}`);
  return voiceId;
};

// Safari-compatible AudioContext playback for ultimate compatibility
export const playAudioWithContext = async (audioBlob: Blob | string): Promise<void> => {
  let audioContext: AudioContext | null = null;
  
  try {
    console.log("üçé [AudioContext] === SAFARI ULTIMATE DEBUG START ===");
    console.log("üçé [AudioContext] User agent:", navigator.userAgent);
    console.log("üçé [AudioContext] Input type:", audioBlob instanceof Blob ? 'Blob' : 'URL');
    
    let arrayBuffer: ArrayBuffer;
    
    if (audioBlob instanceof Blob) {
      console.log("üçé [AudioContext] Converting Blob to ArrayBuffer...");
      arrayBuffer = await audioBlob.arrayBuffer();
    } else {
      console.log("üçé [AudioContext] Fetching audio from URL:", audioBlob.substring(0, 50) + "...");
      const response = await fetch(audioBlob);
      if (!response.ok) {
        throw new Error(`Failed to fetch audio: ${response.statusText}`);
      }
      arrayBuffer = await response.arrayBuffer();
    }
    
    console.log("üçé [AudioContext] Audio data loaded, size:", arrayBuffer.byteLength, "bytes");
    
    // Create AudioContext (Safari-safe way) - CRITICAL FOR SAFARI
    const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
    if (!AudioContextClass) {
      throw new Error("AudioContext not supported in this browser");
    }
    
    audioContext = new AudioContextClass();
    console.log("üçé [AudioContext] AudioContext created:");
    console.log("üçé [AudioContext]   - State:", audioContext.state);
    console.log("üçé [AudioContext]   - Sample rate:", audioContext.sampleRate);
    console.log("üçé [AudioContext]   - Destination:", audioContext.destination);
    
    // CRITICAL: Resume context if suspended (Safari always starts suspended)
    if (audioContext.state === 'suspended') {
      console.log("üçé [AudioContext] ‚ö†Ô∏è Context is SUSPENDED, resuming...");
      await audioContext.resume();
      console.log("üçé [AudioContext] ‚úÖ Context resumed, new state:", audioContext.state);
    } else {
      console.log("üçé [AudioContext] ‚úÖ Context already running");
    }
    
    // Force resume one more time for Safari (belt and suspenders)
    if (audioContext.state !== 'running') {
      console.log("üçé [AudioContext] üîÑ Forcing second resume attempt...");
      await audioContext.resume();
      console.log("üçé [AudioContext] State after force resume:", audioContext.state);
    }
    
    // Decode audio data
    console.log("üçé [AudioContext] Decoding audio data...");
    const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
    console.log("üçé [AudioContext] ‚úÖ Audio decoded successfully:");
    console.log("üçé [AudioContext]   - Duration:", audioBuffer.duration, "seconds");
    console.log("üçé [AudioContext]   - Channels:", audioBuffer.numberOfChannels);
    console.log("üçé [AudioContext]   - Sample rate:", audioBuffer.sampleRate);
    console.log("üçé [AudioContext]   - Length:", audioBuffer.length, "samples");
    
    // Create source node
    const source = audioContext.createBufferSource();
    source.buffer = audioBuffer;
    console.log("üçé [AudioContext] ‚úÖ Buffer source created with audio data");
    
    // Create gain node for MAXIMUM volume control
    const gainNode = audioContext.createGain();
    gainNode.gain.setValueAtTime(1.0, audioContext.currentTime); // Use setValueAtTime for Safari
    console.log("üçé [AudioContext] ‚úÖ Gain node created with volume:", gainNode.gain.value);
    
    // CRITICAL SAFARI STEP: Connect audio graph properly
    console.log("üçé [AudioContext] üîó Connecting audio graph:");
    console.log("üçé [AudioContext] üîó source -> gainNode");
    source.connect(gainNode);
    console.log("üçé [AudioContext] üîó gainNode -> audioContext.destination");
    gainNode.connect(audioContext.destination);
    console.log("üçé [AudioContext] ‚úÖ Audio graph connected successfully");
    
    // Debug destination info
    console.log("üçé [AudioContext] Destination info:");
    console.log("üçé [AudioContext]   - Max channels:", audioContext.destination.maxChannelCount);
    console.log("üçé [AudioContext]   - Channel count:", audioContext.destination.channelCount);
    
    // Return promise that resolves when playback ends
    return new Promise((resolve, reject) => {
      let playbackStarted = false;
      let playbackEnded = false;
      
      // Critical Safari debugging events
      source.onended = () => {
        if (playbackEnded) return; // Prevent double firing
        playbackEnded = true;
        console.log("üçé [AudioContext] üéâ PLAYBACK COMPLETED SUCCESSFULLY!");
        console.log("üçé [AudioContext] Audio actually played to completion");
        
        // Clean up context
        if (audioContext && audioContext.state !== 'closed') {
          audioContext.close();
          console.log("üçé [AudioContext] AudioContext closed");
        }
        resolve();
      };
      
      // Note: AudioBufferSourceNode doesn't have onerror property
      // Errors are typically handled via try-catch blocks around AudioContext operations
      
      // Add debugging event to confirm start
      source.addEventListener('start', () => {
        console.log("üçé [AudioContext] üéµ Source start event fired");
      });
      
      // Set a timeout to check if audio actually played
      const timeoutMs = (audioBuffer.duration + 1) * 1000; // Duration + 1 second buffer
      const timeoutId = setTimeout(() => {
        if (!playbackEnded) {
          console.warn("üçé [AudioContext] ‚ö†Ô∏è Audio did not complete in expected time");
          console.warn("üçé [AudioContext] Expected duration:", audioBuffer.duration, "seconds");
          console.warn("üçé [AudioContext] This might indicate silent playback in Safari");
          
          // Don't reject, just resolve since this is common in Safari
          if (!playbackEnded) {
            playbackEnded = true;
            resolve();
          }
        }
      }, timeoutMs);
      
      // Clear timeout if playback ends normally
      const originalEnded = source.onended;
      source.onended = (event) => {
        clearTimeout(timeoutId);
        if (originalEnded) originalEnded.call(source, event);
      };
      
      // CRITICAL: Start playback
      console.log("üçé [AudioContext] üöÄ STARTING PLAYBACK NOW...");
      if (audioContext) {
        console.log("üçé [AudioContext] Context state before start:", audioContext.state);
      }
      
      try {
        source.start(0);
        playbackStarted = true;
        console.log("üçé [AudioContext] ‚úÖ source.start(0) called successfully");
        console.log("üçé [AudioContext] üéµ Audio should now be playing...");
        
        // Log current time to verify context is active
        if (audioContext) {
          console.log("üçé [AudioContext] Current time:", audioContext.currentTime);
        }
        
        // Monitor context state every 100ms for debugging
        const stateMonitor = setInterval(() => {
          if (playbackEnded) {
            clearInterval(stateMonitor);
            return;
          }
          console.log("üçé [AudioContext] Monitor - State:", audioContext?.state, "Time:", audioContext?.currentTime);
        }, 100);
        
        // Clear monitor after expected duration
        setTimeout(() => clearInterval(stateMonitor), timeoutMs);
        
      } catch (startError) {
        console.error("üçé [AudioContext] ‚ùå source.start() failed:", startError);
        clearTimeout(timeoutId);
        if (audioContext && audioContext.state !== 'closed') {
          audioContext.close();
        }
        reject(startError);
      }
    });
    
  } catch (error) {
    console.error("üçé [AudioContext] ‚ùå FATAL ERROR:", error);
    console.error("üçé [AudioContext] Error details:", {
      name: error instanceof Error ? error.name : 'Unknown',
      message: error instanceof Error ? error.message : String(error),
      stack: error instanceof Error ? error.stack : undefined
    });
    
    // Clean up on error
    if (audioContext && audioContext.state !== 'closed') {
      audioContext.close();
    }
    
    throw error;
  }
};

// Enhanced Audio element playback with proper loading and Safari checks
export const playAudioWithElement = async (audioUrl: string): Promise<void> => {
  return new Promise((resolve, reject) => {
    const audio = new Audio();
    
    // Critical Safari settings
    audio.volume = 1.0;
    audio.muted = false;
    audio.preload = 'auto';
    
    console.log(`üéµ [Audio Element] Setting up audio: ${audioUrl.substring(0, 50)}...`);
    console.log(`üéµ [Audio Element] Safari: ${isSafari()}, Volume: ${audio.volume}, Muted: ${audio.muted}`);
    
    let playbackStarted = false;
    
    audio.oncanplaythrough = () => {
      console.log("üéµ [Audio Element] Can play through - audio fully loaded");
      if (!playbackStarted) {
        playbackStarted = true;
        audio.play().catch((error) => {
          console.error("‚ùå [Audio Element] Play failed in canplaythrough:", error);
          reject(error);
        });
      }
    };
    
    audio.onloadeddata = () => {
      console.log("üéµ [Audio Element] Audio data loaded");
    };
    
    audio.onplay = () => {
      console.log("üéµ [Audio Element] Audio started playing");
    };
    
    audio.onended = () => {
      console.log("‚úÖ [Audio Element] Audio playback completed");
      resolve();
    };
    
    audio.onerror = (error) => {
      console.error("‚ùå [Audio Element] Audio error:", error, audio.error);
      reject(new Error(`Audio playback failed: ${audio.error?.message || 'Unknown error'}`));
    };
    
    // Set source and load
    audio.src = audioUrl;
    audio.load(); // Explicitly load the audio
    
    // Fallback: try to play after a short delay if canplaythrough doesn't fire
    setTimeout(() => {
      if (!playbackStarted && audio.readyState >= 3) { // HAVE_FUTURE_DATA
        console.log("üéµ [Audio Element] Fallback play attempt");
        playbackStarted = true;
        audio.play().catch((error) => {
          console.error("‚ùå [Audio Element] Fallback play failed:", error);
          reject(error);
        });
      }
    }, 1000);
  });
};

// User-triggered audio playback with multiple fallback strategies
export const playAudioFromUserClick = async (audioUrl: string): Promise<void> => {
  console.log(`üéµ [User-triggered] Starting playback: ${audioUrl.substring(0, 50)}...`);
  console.log(`üéµ [User-triggered] Safari detected: ${isSafari()}`);
  
  try {
    // Strategy 1: Try AudioContext first for Safari (most reliable)
    if (isSafari()) {
      console.log("üçé [User-triggered] Using AudioContext for Safari");
      await playAudioWithContext(audioUrl);
      return;
    }
    
    // Strategy 2: Enhanced Audio element for other browsers
    console.log("üéµ [User-triggered] Using enhanced Audio element");
    await playAudioWithElement(audioUrl);
    
  } catch (error) {
    console.error("‚ùå [User-triggered] Primary playback failed:", error);
    
    // Strategy 3: Fallback to simple audio play
    console.log("üéµ [User-triggered] Trying fallback simple audio play");
    try {
      const audio = new Audio(audioUrl);
      audio.volume = 1.0;
      audio.muted = false;
      await audio.play();
      console.log("‚úÖ [User-triggered] Fallback playback successful");
    } catch (fallbackError) {
      console.error("‚ùå [User-triggered] All playback strategies failed:", fallbackError);
      throw fallbackError;
    }
  }
};

export const playUserVoice = async (
  text: string,
  userVoice: string,
  autoplay: boolean = false
): Promise<string | null> => {
  const voiceId = getVoiceId(userVoice);
  console.log(`üé§ Generating voice for ${userVoice} using voice ID: ${voiceId}`);
  try {
    const audioUrl = await speakWithElevenLabs(text, voiceId, autoplay);
    return audioUrl;
  } catch (error) {
    console.error("Error in playUserVoice, returning null:", error);
    return null;
  }
};

// Detect Safari browser for autoplay restrictions
const isSafari = (): boolean => {
  return /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
};

// Safari-compatible audio playback that respects autoplay restrictions
export const playSafariCompatibleAudio = async (
  audioUrl: string,
  requireUserInteraction: boolean = false
): Promise<void> => {
  return new Promise((resolve, reject) => {
    const audio = new Audio(audioUrl);
    audio.volume = 1.0;
    audio.muted = false;
    
    console.log(`üéµ [Safari-compatible] Playing audio: ${audioUrl.substring(0, 50)}...`);
    console.log(`üéµ [Safari-compatible] Safari detected: ${isSafari()}`);
    console.log(`üéµ [Safari-compatible] User interaction required: ${requireUserInteraction}`);
    
    // Event handlers
    audio.oncanplay = () => {
      console.log("üéµ [Safari-compatible] Audio can play");
    };
    
    audio.onplay = () => {
      console.log("üéµ [Safari-compatible] Audio play event fired");
    };
    
    audio.onended = () => {
      console.log("üéµ [Safari-compatible] Audio playback ended successfully");
      resolve();
    };
    
    audio.onerror = (error) => {
      console.error("‚ùå [Safari-compatible] Audio playback error:", error);
      console.error("‚ùå [Safari-compatible] Audio error details:", audio.error);
      reject(new Error("Audio playback failed"));
    };

    // For Safari, we need special handling for autoplay restrictions
    if (isSafari() && requireUserInteraction) {
      console.log("üçé [Safari-compatible] Attempting playback with user interaction handling");
    }
    
    // Attempt to play
    const playPromise = audio.play();
    if (playPromise !== undefined) {
      playPromise
        .then(() => {
          console.log("‚úÖ [Safari-compatible] Audio playback started successfully");
        })
        .catch((error) => {
          console.error("‚ùå [Safari-compatible] Play promise rejected:", error);
          // In Safari, this is often due to autoplay restrictions
          if (error.name === 'NotAllowedError') {
            console.warn("üçé [Safari-compatible] Autoplay blocked - user interaction required");
            reject(new Error("Autoplay blocked: user interaction required"));
          } else {
            reject(new Error(`Audio playback failed: ${error.message}`));
          }
        });
    }
  });
};

export const speakWithElevenLabs = async (
  text: string,
  voiceId: string,
  autoplay: boolean = false
): Promise<string> => {
  try {
    const response = await fetch("/api/elevenlabs-tts", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        text,
        voiceId,
      }),
    });

    if (!response.ok) {
      throw new Error(`Failed to generate speech: ${response.statusText}`);
    }

    const audioBlob = await response.blob();
    console.log("üéß Audio blob type:", audioBlob.type);
    console.log("üéß Audio blob size:", audioBlob.size, "bytes");
    
    // Create correct audio blob with mpeg type for ElevenLabs
    const correctBlob = new Blob([audioBlob], { type: 'audio/mpeg' });
    const audioUrl = URL.createObjectURL(correctBlob);
    console.log("üîä Audio URL generated:", audioUrl.substring(0, 50) + "...");

    if (autoplay) {
      try {
        await playSafariCompatibleAudio(audioUrl, true);
        console.log("‚úÖ ElevenLabs AI voice played successfully");
      } catch (error) {
        console.warn("‚ö†Ô∏è Autoplay failed, audio URL still available for manual playback:", error);
        // Don't throw error, just log warning - the URL is still valid for user-triggered playback
      }
    }
    
    return audioUrl;
  } catch (error) {
    console.error("Error in speakWithElevenLabs:", error);
    throw error; // Re-throw the error to be caught by the caller
  }
};

export const speakText = (
  text: string,
  voiceSettings: AIVoiceSettings
): Promise<void> => {
  return new Promise((resolve, reject) => {
    if ("speechSynthesis" in window) {
      const utterance = new SpeechSynthesisUtterance(text);

      // Set voice properties
      utterance.rate = voiceSettings.rate;
      utterance.pitch = voiceSettings.pitch;
      utterance.volume = voiceSettings.volume;

      // Try to find a matching voice
      const voices = speechSynthesis.getVoices();
      const preferredVoice = voices.find(voice =>
        voice.name.toLowerCase().includes(voiceSettings.voice.toLowerCase())
      );

      if (preferredVoice) {
        utterance.voice = preferredVoice;
      }

      utterance.onend = () => resolve();
      utterance.onerror = error => reject(error);

      speechSynthesis.speak(utterance);
    } else {
      reject(new Error("Speech synthesis not supported"));
    }
  });
};
