// Professional Voice Processing Service
// Integrates multiple APIs for high-quality voice transformation

export interface VoiceProcessingResult {
  success: boolean;
  audioUrl?: string;
  error?: string;
  processingTime?: number;
}

export interface VoiceEffect {
  id: string;
  name: string;
  description: string;
  category: string;
  apiProvider: "elevenlabs" | "local";
  effectId?: string;
  voiceId?: string;
}

// Voicemod API Integration
class VoicemodAPI {
  private apiKey: string;
  private baseUrl = "https://api.voicemod.net";

  constructor(apiKey: string) {
    this.apiKey = apiKey;
    // Test API connection on initialization
    this.testConnection();
  }

  private async testConnection() {
    try {
      console.log("ğŸ§ª Testing Voicemod API connection...");
      const response = await fetch(`${this.baseUrl}/api/v1/effects`, {
        method: "GET",
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          Accept: "application/json",
        },
      });

      console.log(`ğŸ§ª Test response status: ${response.status}`);
      if (response.ok) {
        const data = await response.json();
        console.log("âœ… Voicemod API connection successful!");
        console.log("ğŸ“‹ Available effects:", data);
      } else {
        console.log("âŒ Voicemod API connection failed");
        const errorText = await response.text();
        console.log("ğŸ“‹ Error response:", errorText);
      }
    } catch (error) {
      console.error("ğŸ’¥ Voicemod API test failed:", error);
    }
  }

  async processVoice(
    audioBlob: Blob,
    effectId: string
  ): Promise<VoiceProcessingResult> {
    try {
      const startTime = Date.now();
      console.log(`Processing with Voicemod API, effect: ${effectId}`);

      // Convert blob to base64
      const base64Audio = await this.blobToBase64(audioBlob);

      // Prepare request payload for Voicemod API
      const payload = {
        audio: base64Audio,
        effectId: effectId,
        format: "wav",
        sampleRate: 44100,
      };

      console.log(
        `ğŸš€ Sending request to Voicemod API with effect: ${effectId}`
      );
      console.log(`ğŸ“¡ API URL: ${this.baseUrl}/api/v1/audio/effects`);
      console.log(`ğŸ”‘ Using API Key: ${this.apiKey.substring(0, 10)}...`);
      console.log(`ğŸ“¦ Payload:`, payload);

      const response = await fetch(`${this.baseUrl}/api/v1/audio/effects`, {
        method: "POST",
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
          "Content-Type": "application/json",
          "User-Agent": "VzMVP/1.0",
          Accept: "application/json",
        },
        body: JSON.stringify(payload),
      });

      console.log(`ğŸ“Š Voicemod API response status: ${response.status}`);
      console.log(
        `ğŸ“‹ Response headers:`,
        Object.fromEntries(response.headers.entries())
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`Voicemod API error response: ${errorText}`);
        throw new Error(
          `Voicemod API error: ${response.status} - ${errorText}`
        );
      }

      const result = await response.json();
      console.log("Voicemod API response:", result);

      const processingTime = Date.now() - startTime;

      // Handle different response formats
      let audioUrl: string;
      if (result.processedAudioUrl) {
        audioUrl = result.processedAudioUrl;
      } else if (result.audio) {
        // If API returns base64 audio directly
        audioUrl = `data:audio/wav;base64,${result.audio}`;
      } else if (result.url) {
        audioUrl = result.url;
      } else {
        throw new Error("No audio data received from Voicemod API");
      }

      return {
        success: true,
        audioUrl,
        processingTime,
      };
    } catch (error) {
      console.error("Voicemod API error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  private async blobToBase64(blob: Blob): Promise<string> {
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onload = () => {
        const result = reader.result as string;
        // Remove data URL prefix to get just the base64 data
        const base64 = result.split(",")[1];
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }
}

// ElevenLabs API Integration
class ElevenLabsAPI {
  private apiKey: string;
  private baseUrl = "https://api.elevenlabs.io/v1";

  constructor(apiKey: string) {
    this.apiKey = apiKey;
    // Test API connection on initialization
    this.testConnection();
  }

  private async testConnection() {
    try {
      console.log("ğŸ§ª Testing ElevenLabs API connection...");
      console.log("ğŸ”‘ Using API Key:", this.apiKey.substring(0, 10) + "...");

      const response = await fetch(`${this.baseUrl}/voices`, {
        method: "GET",
        headers: {
          "xi-api-key": this.apiKey,
          Accept: "application/json",
        },
      });

      console.log(`ğŸ§ª ElevenLabs test response status: ${response.status}`);
      console.log(
        `ğŸ§ª Response headers:`,
        Object.fromEntries(response.headers.entries())
      );

      if (response.ok) {
        const data = await response.json();
        console.log("âœ… ElevenLabs API connection successful!");
        console.log("ğŸ“‹ Available voices:", data.voices?.length || 0, "voices");
        if (data.voices && data.voices.length > 0) {
          console.log(
            "ğŸ“‹ First voice:",
            data.voices[0].name,
            "ID:",
            data.voices[0].voice_id
          );
        }
      } else {
        console.log("âŒ ElevenLabs API connection failed");
        const errorText = await response.text();
        console.log("ğŸ“‹ Error response:", errorText);
      }
    } catch (error) {
      console.error("ğŸ’¥ ElevenLabs API test failed:", error);
    }
  }

  async processVoice(
    audioBlob: Blob,
    voiceId: string
  ): Promise<VoiceProcessingResult> {
    try {
      const startTime = Date.now();
      console.log(
        `ğŸ¤ Processing with ElevenLabs Enhanced TTS, voice: ${voiceId}`
      );
      console.log(
        `ğŸ“Š Audio blob size: ${audioBlob.size} bytes, type: ${audioBlob.type}`
      );

      // First, convert audio to text using speech-to-text
      console.log("ğŸ”¤ Converting audio to text...");
      const transcription = await this.speechToText(audioBlob);

      if (!transcription.success || !transcription.text) {
        throw new Error("Failed to transcribe audio");
      }

      console.log("ğŸ“ Transcribed text:", transcription.text);

      // Analyze original audio to determine speed and characteristics
      console.log("ğŸµ Analyzing original audio characteristics...");
      const audioAnalysis = await this.analyzeAudio(audioBlob);
      console.log("ğŸ“Š Audio analysis:", audioAnalysis);

      // Generate new voice using text-to-speech with enhanced settings
      console.log(`ğŸµ Generating speech with voice ID: ${voiceId}`);
      const response = await fetch(
        `${this.baseUrl}/text-to-speech/${voiceId}`,
        {
          method: "POST",
          headers: {
            "xi-api-key": this.apiKey,
            "Content-Type": "application/json",
          },
          body: JSON.stringify({
            text: transcription.text,
            model_id: "eleven_multilingual_v2",
            voice_settings: {
              stability: 0.3, // Lower stability for more natural variation
              similarity_boost: 0.85, // Higher similarity for better voice matching
              style: 0.0, // No style modification
              use_speaker_boost: true, // Enable speaker boost
              speed: audioAnalysis.speed, // Use analyzed speed from original audio
            },
          }),
        }
      );

      console.log(
        `ğŸ“Š ElevenLabs Enhanced TTS response status: ${response.status}`
      );

      if (!response.ok) {
        const errorText = await response.text();
        console.error(`ElevenLabs Enhanced TTS error response: ${errorText}`);
        throw new Error(
          `ElevenLabs Enhanced TTS error: ${response.status} - ${errorText}`
        );
      }

      const audioBuffer = await response.arrayBuffer();
      const audioUrl = URL.createObjectURL(
        new Blob([audioBuffer], { type: "audio/mpeg" })
      );
      const processingTime = Date.now() - startTime;

      console.log(
        `âœ… ElevenLabs Enhanced TTS completed in ${processingTime}ms`
      );

      return {
        success: true,
        audioUrl,
        processingTime,
      };
    } catch (error) {
      console.error("ElevenLabs Enhanced TTS error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  private async analyzeAudio(
    audioBlob: Blob
  ): Promise<{ speed: number; duration: number; wordCount: number }> {
    try {
      // Convert audio to AudioBuffer for analysis
      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

      // Get audio duration
      const duration = audioBuffer.duration;

      // Estimate word count (rough approximation: 150 words per minute)
      const estimatedWordsPerMinute = 150;
      const wordCount = Math.round((duration / 60) * estimatedWordsPerMinute);

      // Calculate speed based on duration and word count
      // Normal speed is around 1.0, faster speech > 1.0, slower speech < 1.0
      const normalWordsPerMinute = 150;
      const actualWordsPerMinute = (wordCount / duration) * 60;
      const speed = Math.max(
        0.5,
        Math.min(2.0, actualWordsPerMinute / normalWordsPerMinute)
      );

      console.log(`ğŸ“Š Audio Analysis Results:`);
      console.log(`   Duration: ${duration.toFixed(2)}s`);
      console.log(`   Estimated words: ${wordCount}`);
      console.log(`   Words per minute: ${actualWordsPerMinute.toFixed(1)}`);
      console.log(`   Calculated speed: ${speed.toFixed(2)}`);

      return {
        speed: speed,
        duration: duration,
        wordCount: wordCount,
      };
    } catch (error) {
      console.error("Audio analysis error:", error);
      // Return default values if analysis fails
      return {
        speed: 1.0,
        duration: 0,
        wordCount: 0,
      };
    }
  }

  private async speechToText(
    audioBlob: Blob
  ): Promise<{ success: boolean; text?: string; error?: string }> {
    try {
      console.log("ğŸ”¤ Starting speech-to-text conversion...");
      console.log(
        "ğŸ“Š Audio blob for STT:",
        audioBlob.size,
        "bytes, type:",
        audioBlob.type
      );

      // Convert audio to the correct format for ElevenLabs STT
      let audioFile: Blob;
      if (audioBlob.type === "audio/mp4") {
        // Convert MP4 to WAV format for better compatibility
        const audioContext = new (window.AudioContext ||
          (window as any).webkitAudioContext)();
        const arrayBuffer = await audioBlob.arrayBuffer();
        const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);

        // Create WAV blob
        const wavBlob = await this.audioBufferToWav(audioBuffer);
        audioFile = wavBlob;
      } else {
        audioFile = audioBlob;
      }

      const formData = new FormData();
      formData.append("file", audioFile, "audio.wav");
      formData.append("model_id", "scribe_v1");

      console.log(
        "ğŸ“¡ Sending STT request to:",
        `${this.baseUrl}/speech-to-text`
      );
      console.log(
        "ğŸ“Š Audio file size:",
        audioFile.size,
        "bytes, type:",
        audioFile.type
      );

      const response = await fetch(`${this.baseUrl}/speech-to-text`, {
        method: "POST",
        headers: {
          "xi-api-key": this.apiKey,
        },
        body: formData,
      });

      console.log("ğŸ“Š STT response status:", response.status);

      if (!response.ok) {
        const errorText = await response.text();
        console.error("âŒ STT error response:", errorText);
        throw new Error(
          `Speech-to-text error: ${response.status} - ${errorText}`
        );
      }

      const result = await response.json();
      console.log("âœ… STT result:", result);

      return {
        success: true,
        text: result.text,
      };
    } catch (error) {
      console.error("ğŸ’¥ STT error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  private async audioBufferToWav(audioBuffer: AudioBuffer): Promise<Blob> {
    const numChannels = audioBuffer.numberOfChannels;
    const sampleRate = audioBuffer.sampleRate;
    const length = audioBuffer.length;

    // Create WAV header
    const buffer = new ArrayBuffer(44 + length * numChannels * 2);
    const view = new DataView(buffer);

    // WAV header
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };

    writeString(0, "RIFF");
    view.setUint32(4, 36 + length * numChannels * 2, true);
    writeString(8, "WAVE");
    writeString(12, "fmt ");
    view.setUint32(16, 16, true);
    view.setUint16(20, 1, true);
    view.setUint16(22, numChannels, true);
    view.setUint32(24, sampleRate, true);
    view.setUint32(28, sampleRate * numChannels * 2, true);
    view.setUint16(32, numChannels * 2, true);
    view.setUint16(34, 16, true);
    writeString(36, "data");
    view.setUint32(40, length * numChannels * 2, true);

    // Write audio data
    let offset = 44;
    for (let i = 0; i < length; i++) {
      for (let channel = 0; channel < numChannels; channel++) {
        const sample = Math.max(
          -1,
          Math.min(1, audioBuffer.getChannelData(channel)[i])
        );
        view.setInt16(
          offset,
          sample < 0 ? sample * 0x8000 : sample * 0x7fff,
          true
        );
        offset += 2;
      }
    }

    return new Blob([buffer], { type: "audio/wav" });
  }
}

// Azure Cognitive Services Integration
class AzureVoiceAPI {
  private apiKey: string;
  private region: string;
  private baseUrl: string;

  constructor(apiKey: string, region: string) {
    this.apiKey = apiKey;
    this.region = region;
    this.baseUrl = `https://${region}.tts.speech.microsoft.com/cognitiveservices/v1`;
  }

  async processVoice(
    audioBlob: Blob,
    voiceName: string
  ): Promise<VoiceProcessingResult> {
    try {
      const startTime = Date.now();

      // Convert audio to text
      const transcription = await this.speechToText(audioBlob);

      if (!transcription.success || !transcription.text) {
        throw new Error("Failed to transcribe audio");
      }

      // Generate new voice
      const response = await fetch(`${this.baseUrl}/text-to-speech`, {
        method: "POST",
        headers: {
          "Ocp-Apim-Subscription-Key": this.apiKey,
          "Content-Type": "application/ssml+xml",
          "X-Microsoft-OutputFormat": "audio-16khz-128kbitrate-mono-mp3",
        },
        body: this.createSSML(transcription.text, voiceName),
      });

      if (!response.ok) {
        throw new Error(`Azure API error: ${response.status}`);
      }

      const audioBuffer = await response.arrayBuffer();
      const audioUrl = URL.createObjectURL(
        new Blob([audioBuffer], { type: "audio/mpeg" })
      );
      const processingTime = Date.now() - startTime;

      return {
        success: true,
        audioUrl,
        processingTime,
      };
    } catch (error) {
      console.error("Azure API error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  private async speechToText(
    audioBlob: Blob
  ): Promise<{ success: boolean; text?: string; error?: string }> {
    try {
      const response = await fetch(
        `https://${this.region}.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1`,
        {
          method: "POST",
          headers: {
            "Ocp-Apim-Subscription-Key": this.apiKey,
            "Content-Type": "audio/wav",
          },
          body: audioBlob,
        }
      );

      if (!response.ok) {
        throw new Error(`Speech-to-text error: ${response.status}`);
      }

      const result = await response.json();
      return {
        success: true,
        text: result.DisplayText,
      };
    } catch (error) {
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  private createSSML(text: string, voiceName: string): string {
    return `<speak version='1.0' xml:lang='en-US'>
      <voice xml:lang='en-US' xml:gender='Female' name='${voiceName}'>
        ${text}
      </voice>
    </speak>`;
  }
}

// Enhanced Local Processing with Advanced Audio Effects
class LocalVoiceProcessor {
  async processVoice(
    audioBlob: Blob,
    effectId: string
  ): Promise<VoiceProcessingResult> {
    try {
      console.log(
        `ğŸµ LocalVoiceProcessor: Starting processing for effect: ${effectId}`
      );
      console.log(
        `ğŸ“Š Audio blob info: size=${audioBlob.size}, type=${audioBlob.type}`
      );

      const startTime = Date.now();

      const audioContext = new (window.AudioContext ||
        (window as any).webkitAudioContext)();
      console.log(
        `ğŸµ AudioContext created: sampleRate=${audioContext.sampleRate}`
      );

      const arrayBuffer = await audioBlob.arrayBuffer();
      console.log(`ğŸ“Š ArrayBuffer size: ${arrayBuffer.byteLength} bytes`);

      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
      console.log(
        `ğŸµ AudioBuffer decoded: duration=${audioBuffer.duration}s, channels=${audioBuffer.numberOfChannels}, sampleRate=${audioBuffer.sampleRate}`
      );

      // Create advanced processing chain
      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;

      // Apply professional effects based on effectId
      console.log(`ğŸ­ Applying professional effects for: ${effectId}`);
      const processedAudio = await this.applyProfessionalEffects(
        audioContext,
        source,
        effectId
      );

      const processingTime = Date.now() - startTime;
      console.log(
        `âœ… LocalVoiceProcessor: Processing completed in ${processingTime}ms`
      );
      console.log(`ğŸµ Processed audio URL: ${processedAudio}`);

      return {
        success: true,
        audioUrl: processedAudio,
        processingTime,
      };
    } catch (error) {
      console.error("âŒ LocalVoiceProcessor error:", error);
      console.error("âŒ Error details:", {
        message: error instanceof Error ? error.message : "Unknown error",
        stack: error instanceof Error ? error.stack : "No stack trace",
        effectId: effectId,
        blobSize: audioBlob.size,
        blobType: audioBlob.type,
      });
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  private async applyProfessionalEffects(
    audioContext: AudioContext,
    source: AudioBufferSourceNode,
    effectId: string
  ): Promise<string> {
    console.log(
      `ğŸ­ applyProfessionalEffects: Starting for effect: ${effectId}`
    );

    // Create advanced audio processing chain with pitch shifting
    const filters = this.createFilterChain(audioContext, effectId);
    const compressor = this.createCompressor(audioContext);
    const gainNode = audioContext.createGain();

    // Apply dramatic pitch shifting for real voice transformation
    const pitchSettings = this.getPitchSettings(effectId);
    source.playbackRate.value = pitchSettings.playbackRate;
    console.log(
      `ğŸµ Applied pitch shift: ${pitchSettings.playbackRate}x playback rate (${pitchSettings.description})`
    );

    // Set gain for more natural effect
    gainNode.gain.value = 1.0; // Keep original volume
    console.log(`ğŸ”Š Applied gain: 1.0x (original volume)`);

    // Connect the advanced processing chain
    console.log(`ğŸ”— Connecting audio processing chain...`);
    source.connect(filters.input);
    filters.output.connect(compressor);
    compressor.connect(gainNode);

    // Record the processed audio
    const mediaStreamDestination = audioContext.createMediaStreamDestination();
    gainNode.connect(mediaStreamDestination);
    console.log(
      `ğŸ™ï¸ MediaStreamDestination connected, stream tracks: ${mediaStreamDestination.stream.getTracks().length}`
    );

    // Determine Safari-compatible MIME type for MediaRecorder
    function isSafari(): boolean {
      const userAgent = navigator.userAgent;
      return userAgent.includes("Safari") && !userAgent.includes("Chrome");
    }

    function getCompatibleMimeType(): string {
      if (isSafari()) {
        // Safari prefers MP4/AAC for recording and playback
        if (MediaRecorder.isTypeSupported("audio/mp4")) {
          return "audio/mp4";
        } else if (MediaRecorder.isTypeSupported("audio/mp4;codecs=mp4a.40.2")) {
          return "audio/mp4;codecs=mp4a.40.2";
        } else {
          return "audio/mp4"; // fallback
        }
      } else {
        // Chrome/Firefox prefer WebM
        if (MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) {
          return "audio/webm;codecs=opus";
        } else if (MediaRecorder.isTypeSupported("audio/webm")) {
          return "audio/webm";
        } else {
          return "audio/webm"; // fallback
        }
      }
    }

    const mimeType = getCompatibleMimeType();
    console.log(`ğŸ¤ Using MIME type for LocalVoiceProcessor: ${mimeType}`);

    const mediaRecorder = new MediaRecorder(mediaStreamDestination.stream, { mimeType });
    const chunks: Blob[] = [];

    return new Promise(resolve => {
      mediaRecorder.ondataavailable = event => {
        console.log(
          `ğŸ“¦ Recording chunk: ${event.data.size} bytes, type: ${event.data.type}`
        );
        chunks.push(event.data);
      };
      mediaRecorder.onstop = () => {
        console.log(`ğŸ›‘ MediaRecorder stopped, total chunks: ${chunks.length}`);

        const blob = new Blob(chunks, { type: mimeType });
        console.log(
          `âœ… Processing complete. Total size: ${blob.size} bytes, type: ${mimeType}`
        );
        const audioUrl = URL.createObjectURL(blob);
        console.log("ğŸµ Generated audio URL:", audioUrl);
        resolve(audioUrl);
      };

      console.log(`â–¶ï¸ Starting MediaRecorder and audio source...`);
      mediaRecorder.start();
      source.start(0);

      const duration = source.buffer!.duration * 1000 + 500;
      console.log(
        `â±ï¸ Audio duration: ${source.buffer!.duration}s, will stop in ${duration}ms`
      );

      setTimeout(() => {
        console.log(`â¹ï¸ Stopping MediaRecorder and audio source...`);
        mediaRecorder.stop();
        source.stop();
      }, duration);
    });
  }

  public getPitchSettings(effectId: string) {
    const pitchConfigs = {
      // Local Effects
      "sweet-angel": {
        playbackRate: 1.2,
        description: "Slightly higher pitch for female transformation",
      },
      "elegant-queen": {
        playbackRate: 1.2,
        description: "Medium-high pitch for mature female",
      },
      "noble-knight": {
        playbackRate: 0.7,
        description: "Lower pitch for male transformation",
      },
      "wise-sage": {
        playbackRate: 0.6,
        description: "Very low pitch for elder voice",
      },
      "crystal-clear": {
        playbackRate: 1.0,
        description: "Normal pitch with enhanced clarity",
      },
      "mysterious-siren": {
        playbackRate: 0.85,
        description: "Mysterious female voice",
      },
      "energetic-sunshine": {
        playbackRate: 1.25,
        description: "Energetic and bright voice",
      },
      "adventurous-explorer": {
        playbackRate: 0.9,
        description: "Adventurous male voice",
      },
      "gentle-artist": {
        playbackRate: 1.0,
        description: "Gentle and artistic voice",
      },
      "warm-embrace": {
        playbackRate: 0.95,
        description: "Warm and comforting voice",
      },
      "powerful-command": {
        playbackRate: 0.8,
        description: "Powerful and commanding voice",
      },
      "voicemod-echo": {
        playbackRate: 1.0,
        description: "Echo effect - Enhanced local",
      },
      "voicemod-radio": {
        playbackRate: 1.1,
        description: "Radio effect - Enhanced local",
      },
      "voicemod-phone": {
        playbackRate: 0.9,
        description: "Phone effect - Enhanced local",
      },

      // ElevenLabs effects with enhanced local processing
      "elevenlabs-rachel": {
        playbackRate: 1.3,
        description: "Rachel voice - Enhanced local",
      },
      "elevenlabs-domi": {
        playbackRate: 0.8,
        description: "Domi voice - Enhanced local",
      },
      "elevenlabs-bella": {
        playbackRate: 1.4,
        description: "Bella voice - Enhanced local",
      },
      "elevenlabs-arnold": {
        playbackRate: 0.6,
        description: "Arnold voice - Enhanced local",
      },
    };
    return (
      pitchConfigs[effectId as keyof typeof pitchConfigs] || {
        playbackRate: 1.0,
        description: "Default pitch",
      }
    );
  }

  private createFilterChain(audioContext: AudioContext, effectId: string) {
    console.log(`ğŸ›ï¸ Creating filter chain for effect: ${effectId}`);
    
    // Create a simple filter chain
    const inputGain = audioContext.createGain();
    const outputGain = audioContext.createGain();
    
    // Set up basic filter
    inputGain.gain.value = 1.0;
    outputGain.gain.value = 1.0;
    
    // Connect input to output
    inputGain.connect(outputGain);
    
    return {
      input: inputGain,
      output: outputGain
    };
  }

  private createCompressor(audioContext: AudioContext) {
    console.log(`ğŸšï¸ Creating compressor`);
    
    const compressor = audioContext.createDynamicsCompressor();
    compressor.threshold.value = -24;
    compressor.knee.value = 30;
    compressor.ratio.value = 12;
    compressor.attack.value = 0.003;
    compressor.release.value = 0.25;
    
    return compressor;
  }
}

// Main Voice Processing Service
export class VoiceProcessingService {
  private voicemodAPI?: VoicemodAPI;
  private elevenLabsAPI?: ElevenLabsAPI;
  private azureAPI?: AzureVoiceAPI;
  private localProcessor: LocalVoiceProcessor;

  constructor() {
    this.localProcessor = new LocalVoiceProcessor();

    // APIå¯†é’¥ç°åœ¨é€šè¿‡å®‰å…¨çš„åç«¯APIè·¯ç”±å¤„ç†ï¼Œä¸å†åœ¨å‰ç«¯æš´éœ²
    const voicemodKey = null;
    const elevenLabsKey = null;
    const azureKey = null;
    const azureRegion = null;

    // APIå¯†é’¥ç°åœ¨é€šè¿‡åç«¯APIè·¯ç”±å¤„ç†ï¼Œä¸å†åœ¨å‰ç«¯åˆå§‹åŒ–APIæœåŠ¡
    // æ‰€æœ‰è¯­éŸ³å¤„ç†éƒ½é€šè¿‡å®‰å…¨çš„åç«¯APIè¿›è¡Œ
  }

  // æ£€æŸ¥åç«¯APIæ˜¯å¦å¯ç”¨ (é€šè¿‡APIè·¯ç”±)
  async checkBackendApiStatus(): Promise<{hasElevenLabs: boolean, hasOpenAI: boolean}> {
    try {
      // æµ‹è¯•ElevenLabs APIè·¯ç”±
      const elevenLabsTest = await fetch('/api/elevenlabs-tts', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ text: 'test', voiceId: '9BWtsMINqrJLrRacOk9x' })
      });
      
      // æµ‹è¯•OpenAI APIè·¯ç”±  
      const openAITest = await fetch('/api/ai-chat', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ message: 'test', personality: 'test', conversationHistory: [] })
      });
      
      return {
        hasElevenLabs: elevenLabsTest.ok,
        hasOpenAI: openAITest.ok
      };
    } catch (error) {
      console.error('Error checking backend API status:', error);
      return { hasElevenLabs: false, hasOpenAI: false };
    }
  }

  // æ£€æŸ¥APIå¯†é’¥çŠ¶æ€ (ç°åœ¨æ£€æŸ¥åç«¯APIå¯ç”¨æ€§)
  hasApiKeys(): boolean {
    // ç”±äºæˆ‘ä»¬ç°åœ¨ä½¿ç”¨åç«¯APIè·¯ç”±ï¼Œæ€»æ˜¯è¿”å›true
    // å®é™…çš„APIçŠ¶æ€æ£€æŸ¥é€šè¿‡checkBackendApiStatusè¿›è¡Œ
    return true;
  }

  // è·å–APIçŠ¶æ€è¯¦æƒ…
  getApiStatus() {
    return {
      hasVoicemod: false, // æœªä½¿ç”¨
      hasElevenLabs: true, // é€šè¿‡åç«¯APIè·¯ç”±
      hasAzure: false, // æœªä½¿ç”¨
      hasAnyApi: true // é€šè¿‡åç«¯APIè·¯ç”±
    };
  }

  async processVoice(
    audioBlob: Blob,
    effect: VoiceEffect
  ): Promise<VoiceProcessingResult> {
    console.log(
      `ğŸ¤ Processing voice with ${effect.apiProvider} for effect: ${effect.name}`
    );
    console.log(
      `ğŸ“Š Audio blob size: ${audioBlob.size} bytes, type: ${audioBlob.type}`
    );

    try {
      switch (effect.apiProvider) {
        case "elevenlabs":
          if (this.elevenLabsAPI && effect.voiceId) {
            console.log("ğŸš€ Attempting ElevenLabs API processing...");
            return await this.elevenLabsAPI.processVoice(
              audioBlob,
              effect.voiceId
            );
          } else {
            console.error("âŒ ElevenLabs API not available or missing voiceId");
            return {
              success: false,
              error:
                "ElevenLabs API not available - missing API key or voice configuration",
            };
          }

        case "local":
        default:
          console.log("ğŸš€ Using local processing...");
          return await this.localProcessor.processVoice(audioBlob, effect.id);
      }

      // Fallback to local processing
      console.log("ğŸ”„ Falling back to local processing...");
      return await this.localProcessor.processVoice(audioBlob, effect.id);
    } catch (error) {
      console.error("ğŸ’¥ Voice processing error:", error);
      return {
        success: false,
        error: error instanceof Error ? error.message : "Unknown error",
      };
    }
  }

  getAvailableEffects(): VoiceEffect[] {
    return [
      // Enhanced Local Effects (Fallback) - Put these first for immediate availability
      {
        id: "sweet-angel",
        name: "ğŸ‘¼ Sweet Angel (Local)",
        description: "Higher pitch for female transformation",
        category: "Local Effects",
        apiProvider: "local",
        effectId: "sweet-angel",
      },
      {
        id: "elegant-queen",
        name: "ğŸ‘‘ Elegant Queen (Local)",
        description: "Medium-high pitch for mature female",
        category: "Local Effects",
        apiProvider: "local",
        effectId: "elegant-queen",
      },
      {
        id: "noble-knight",
        name: "âš”ï¸ Noble Knight (Local)",
        description: "Lower pitch for male transformation",
        category: "Local Effects",
        apiProvider: "local",
        effectId: "noble-knight",
      },
      {
        id: "wise-sage",
        name: "ğŸ§™â€â™‚ï¸ Wise Sage (Local)",
        description: "Very low pitch for elder voice",
        category: "Local Effects",
        apiProvider: "local",
        effectId: "wise-sage",
      },
      {
        id: "crystal-clear",
        name: "ğŸ’ Crystal Clear (Local)",
        description: "Enhanced clarity and brightness",
        category: "Local Effects",
        apiProvider: "local",
        effectId: "crystal-clear",
      },

      // ElevenLabs Professional Female Voices
      {
        id: "elevenlabs-rachel",
        name: "ğŸ‘© Rachel",
        description: "Professional female voice - clear and confident",
        category: "Professional Female",
        apiProvider: "elevenlabs",
        voiceId: "21m00Tcm4TlvDq8ikWAM",
      },
      {
        id: "elevenlabs-bella",
        name: "ğŸ‘© Bella",
        description: "Sweet and warm female voice",
        category: "Professional Female",
        apiProvider: "elevenlabs",
        voiceId: "EXAVITQu4vr4xnSDxMaL",
      },
      {
        id: "elevenlabs-aria",
        name: "ğŸ‘© Aria",
        description: "Elegant and sophisticated female voice",
        category: "Professional Female",
        apiProvider: "elevenlabs",
        voiceId: "9BWtsMINqrJLrRacOk9x",
      },
      {
        id: "elevenlabs-sarah",
        name: "ğŸ‘© Sarah",
        description: "Friendly and approachable female voice",
        category: "Professional Female",
        apiProvider: "elevenlabs",
        voiceId: "kdmDKE6EkgrWrrykO9Qt",
      },
      {
        id: "elevenlabs-emily",
        name: "ğŸ‘© Emily",
        description: "Young and energetic female voice",
        category: "Professional Female",
        apiProvider: "elevenlabs",
        voiceId: "g6xIsTj2HwM6VR4iXFCw",
      },

      // ElevenLabs Professional Male Voices
      {
        id: "elevenlabs-domi",
        name: "ğŸ‘¨ Domi",
        description: "Professional male voice - British analytical (Archer)",
        category: "Professional Male",
        apiProvider: "elevenlabs",
        voiceId: "L0Dsvb3SLTyegXwtm47J",
      },
      {
        id: "elevenlabs-arnold",
        name: "ğŸ‘¨ Arnold",
        description: "Deep and powerful male voice",
        category: "Professional Male",
        apiProvider: "elevenlabs",
        voiceId: "pNInz6obpgDQGcFmaJgB",
      },
      {
        id: "elevenlabs-josh",
        name: "ğŸ‘¨ Josh",
        description: "Warm and friendly male voice",
        category: "Professional Male",
        apiProvider: "elevenlabs",
        voiceId: "TxGEqnHWrfWFTfGW9XjX",
      },
      {
        id: "elevenlabs-sam",
        name: "ğŸ‘¨ Sam",
        description: "Young and energetic male voice",
        category: "Professional Male",
        apiProvider: "elevenlabs",
        voiceId: "yoZ06aMxZJJ28mfd3POQ",
      },
      {
        id: "elevenlabs-echo",
        name: "ğŸ‘¨ Echo",
        description: "Deep energetic Australian voice (Stuart)",
        category: "Professional Male",
        apiProvider: "elevenlabs",
        voiceId: "HDA9tsk27wYi3uq0fPcK",
      },

      // ElevenLabs Character Voices
      {
        id: "elevenlabs-dorothy",
        name: "ğŸ‘µ Dorothy",
        description: "Wise and gentle elder voice",
        category: "Character Voices",
        apiProvider: "elevenlabs",
        voiceId: "ThT5KcBeYPX3keUQqHPh",
      },
      {
        id: "elevenlabs-charlie",
        name: "ğŸ‘´ Charlie",
        description: "Mature and experienced male voice",
        category: "Character Voices",
        apiProvider: "elevenlabs",
        voiceId: "2EiwWnXFnvU5JabPnv8n",
      },
      {
        id: "elevenlabs-lily",
        name: "ğŸ‘§ Lily",
        description: "Young and cheerful female voice",
        category: "Character Voices",
        apiProvider: "elevenlabs",
        voiceId: "EXAVITQu4vr4xnSDxMaL",
      },
      {
        id: "elevenlabs-tommy",
        name: "ğŸ‘¦ Tommy",
        description: "Young and enthusiastic male voice",
        category: "Character Voices",
        apiProvider: "elevenlabs",
        voiceId: "yoZ06aMxZJJ28mfd3POQ",
      },
    ];
  }

  public getPitchSettings(effectId: string) {
    return this.localProcessor.getPitchSettings(effectId);
  }
}

// Export singleton instance
export const voiceProcessingService = new VoiceProcessingService();
